{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beyondllm in c:\\python312\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: youtube_transcript_api in c:\\python312\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: llama-index-readers-youtube-transcript in c:\\python312\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: llama_index.embeddings.huggingface in c:\\python312\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: llama-index==0.10.27 in c:\\python312\\lib\\site-packages (from beyondllm) (0.10.27)\n",
      "Requirement already satisfied: llama-index-embeddings-gemini==0.1.6 in c:\\python312\\lib\\site-packages (from beyondllm) (0.1.6)\n",
      "Requirement already satisfied: nltk==3.8.1 in c:\\python312\\lib\\site-packages (from beyondllm) (3.8.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\python312\\lib\\site-packages (from beyondllm) (1.26.4)\n",
      "Collecting openai==1.20.0 (from beyondllm)\n",
      "  Using cached openai-1.20.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pandas==2.0.3 in c:\\python312\\lib\\site-packages (from beyondllm) (2.0.3)\n",
      "Requirement already satisfied: pydantic<2,>=1.10.5 in c:\\python312\\lib\\site-packages (from beyondllm) (1.10.17)\n",
      "Requirement already satisfied: pypdf==4.2.0 in c:\\python312\\lib\\site-packages (from beyondllm) (4.2.0)\n",
      "Requirement already satisfied: pysbd==0.3.4 in c:\\python312\\lib\\site-packages (from beyondllm) (0.3.4)\n",
      "Requirement already satisfied: pyyaml==6.0.1 in c:\\python312\\lib\\site-packages (from beyondllm) (6.0.1)\n",
      "Requirement already satisfied: regex==2024.4.16 in c:\\python312\\lib\\site-packages (from beyondllm) (2024.4.16)\n",
      "Requirement already satisfied: sqlalchemy==2.0.29 in c:\\python312\\lib\\site-packages (from beyondllm) (2.0.29)\n",
      "Requirement already satisfied: tiktoken==0.6.0 in c:\\python312\\lib\\site-packages (from beyondllm) (0.6.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.2.7)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.1.12)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.27 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.10.66)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.1.10)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.1.6)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.1.25)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.1.7)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.1.27)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in c:\\python312\\lib\\site-packages (from llama-index==0.10.27->beyondllm) (0.1.6)\n",
      "Collecting google-generativeai<0.5.0,>=0.4.1 (from llama-index-embeddings-gemini==0.1.6->beyondllm)\n",
      "  Using cached google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk==3.8.1->beyondllm) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk==3.8.1->beyondllm) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from nltk==3.8.1->beyondllm) (4.66.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python312\\lib\\site-packages (from openai==1.20.0->beyondllm) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python312\\lib\\site-packages (from openai==1.20.0->beyondllm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python312\\lib\\site-packages (from openai==1.20.0->beyondllm) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\python312\\lib\\site-packages (from openai==1.20.0->beyondllm) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\python312\\lib\\site-packages (from openai==1.20.0->beyondllm) (4.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas==2.0.3->beyondllm) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas==2.0.3->beyondllm) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python312\\lib\\site-packages (from pandas==2.0.3->beyondllm) (2024.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python312\\lib\\site-packages (from sqlalchemy==2.0.29->beyondllm) (3.0.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\python312\\lib\\site-packages (from tiktoken==0.6.0->beyondllm) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\python312\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (0.23.4)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\python312\\lib\\site-packages (from llama_index.embeddings.huggingface) (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python312\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\python312\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (24.0)\n",
      "Requirement already satisfied: aiohttp in c:\\python312\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.9.5)\n",
      "Requirement already satisfied: minijinja>=1.0 in c:\\python312\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (2.0.1)\n",
      "Requirement already satisfied: dataclasses-json in c:\\python312\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\python312\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\python312\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (1.0.8)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\okan_\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\python312\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (3.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\python312\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (10.3.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\python312\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (8.3.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\python312\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\python312\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.6.0->beyondllm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.6.0->beyondllm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.6.0->beyondllm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.6.0->beyondllm) (2024.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\python312\\lib\\site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (4.42.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\python312\\lib\\site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.13.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python312\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python312\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python312\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python312\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python312\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.9.4)\n",
      "Collecting google-ai-generativelanguage==0.4.0 (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm)\n",
      "  Using cached google_ai_generativelanguage-0.4.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\python312\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (2.29.0)\n",
      "Requirement already satisfied: google-api-core in c:\\python312\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (2.19.0)\n",
      "Requirement already satisfied: protobuf in c:\\python312\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (1.23.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.20.0->beyondllm) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.20.0->beyondllm) (0.14.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.19 in c:\\python312\\lib\\site-packages (from llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2->llama-index==0.10.27->beyondllm) (0.1.19)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\python312\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27->beyondllm) (4.12.3)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\python312\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27->beyondllm) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.4.0 in c:\\python312\\lib\\site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.27->beyondllm) (0.4.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.0.3->beyondllm) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm->nltk==3.8.1->beyondllm) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (0.19.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python312\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python312\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (3.21.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python312\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27->beyondllm) (2.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\python312\\lib\\site-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (1.63.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (4.9)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.3.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (1.62.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (0.6.0)\n",
      "Using cached openai-1.20.0-py3-none-any.whl (292 kB)\n",
      "Using cached google_generativeai-0.4.1-py3-none-any.whl (137 kB)\n",
      "Using cached google_ai_generativelanguage-0.4.0-py3-none-any.whl (598 kB)\n",
      "Installing collected packages: openai, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: google-ai-generativelanguage\n",
      "    Found existing installation: google-ai-generativelanguage 0.6.6\n",
      "    Uninstalling google-ai-generativelanguage-0.6.6:\n",
      "      Successfully uninstalled google-ai-generativelanguage-0.6.6\n",
      "  Attempting uninstall: google-generativeai\n",
      "    Found existing installation: google-generativeai 0.7.2\n",
      "    Uninstalling google-generativeai-0.7.2:\n",
      "      Successfully uninstalled google-generativeai-0.7.2\n",
      "Successfully installed google-ai-generativelanguage-0.4.0 google-generativeai-0.4.1 openai-1.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~penai (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~penai (C:\\Python312\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "crewai 0.51.1 requires pydantic<3.0.0,>=2.4.2, but you have pydantic 1.10.17 which is incompatible.\n",
      "crewai 0.51.1 requires regex<2024.0.0,>=2023.12.25, but you have regex 2024.4.16 which is incompatible.\n",
      "crewai-tools 0.8.3 requires pydantic<3.0.0,>=2.6.1, but you have pydantic 1.10.17 which is incompatible.\n",
      "embedchain 0.1.120 requires tiktoken<0.8.0,>=0.7.0, but you have tiktoken 0.6.0 which is incompatible.\n",
      "instructor 1.3.3 requires pydantic<3.0.0,>=2.7.0, but you have pydantic 1.10.17 which is incompatible.\n",
      "langchain-google-genai 1.0.8 requires google-generativeai<0.8.0,>=0.7.0, but you have google-generativeai 0.4.1 which is incompatible.\n",
      "langchain-openai 0.1.21 requires openai<2.0.0,>=1.40.0, but you have openai 1.20.0 which is incompatible.\n",
      "langchain-openai 0.1.21 requires tiktoken<1,>=0.7, but you have tiktoken 0.6.0 which is incompatible.\n",
      "mem0ai 0.0.9 requires openai<2.0.0,>=1.33.0, but you have openai 1.20.0 which is incompatible.\n",
      "mem0ai 0.0.9 requires pydantic<3.0.0,>=2.7.3, but you have pydantic 1.10.17 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Gerekli kütüphaneleri yükleme\n",
    "!pip install beyondllm youtube_transcript_api llama-index-readers-youtube-transcript llama_index.embeddings.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python kodu: API Anahtarlarını al ve ortam değişkenlerine ata\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Hugging Face ve Google API anahtarlarını güvenli bir şekilde alma ve ayarlama\n",
    "hf_token = getpass('Enter Your HuggingfaceHub Token')\n",
    "google_api_key = getpass('Enter Your Google API Key')\n",
    "\n",
    "# Çevresel değişkenler olarak ayarlama\n",
    "os.environ['HF_TOKEN'] = hf_token\n",
    "os.environ['GOOGLE_API_KEY'] = google_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=ZM1bdh2mDJQ']\n"
     ]
    }
   ],
   "source": [
    "from beyondllm import source, embeddings, retrieve, llms, generator\n",
    "\n",
    "# YouTube videosundan veriyi yükleme\n",
    "data = source.fit(\n",
    "    path=\"https://www.youtube.com/watch?v=ZM1bdh2mDJQ\",  # Video linki\n",
    "    dtype=\"youtube\",\n",
    "    chunk_size=1024,  # Veriyi parçalara ayırma boyutu\n",
    "    chunk_overlap=0   # Parçalar arasında üst üste binme miktarı\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi vektörlere dönüştürmek için embedding modeli seçme\n",
    "model_name = 'BAAI/bge-small-en-v1.5'  # Hugging Face üzerinde bulunan bir model\n",
    "embed_model = embeddings.HuggingFaceEmbeddings(\n",
    "    model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gelişmiş retriever'ı ayarlama\n",
    "retriever = retrieve.auto_retriever(\n",
    "    data=data,\n",
    "    embed_model=embed_model,\n",
    "    type=\"cross-rerank\",  # Gelişmiş retriever türü\n",
    "    mode=\"OR\",            # 'OR' modunda çalışacak\n",
    "    top_k=2               # En iyi iki eşleşmeyi getir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorgu sonucu bulunan veriler: [NodeWithScore(node=TextNode(id_='36748b5c-0bf2-448d-b0d1-5900691da625', embedding=None, metadata={'video_id': 'ZM1bdh2mDJQ'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ZM1bdh2mDJQ', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'ZM1bdh2mDJQ'}, hash='615e7d33180855d5371f6b37c8848fe6cb5ae545e1ba29af48aae860c75a6437')}, text=\"hi everyone welcome to the part seven\\nvideo of building llm applications using\\ngen stack in this video we are going to\\nbuild a CSV agent using this tack so the\\nCSV file will be the data source for the\\nentire R pipeline so if you do not know\\nwhat rag is then in short it's retrieval\\naugmented generation it streamlines the\\nprocess of retrieving data from a data\\nsource then pre-processing it and\\nstoring it in a certain way in a vector\\nstore and then retri it when a query is\\npassed and then presenting it with the\\nhelp of a llm which we're going to\\nprovide and the llm will generate\\nresponse based on the query which is\\nbeing\\nasked so let's start building the rack\\npipeline first of all we need a loader\\nwhich in this case will be a CSV loader\\nsince we are using a CSV file so let's\\ndrag and drop uh now I'll be using a\\nTitanic data set which I have downloaded\\nfrom kaggle so yeah I'll be using that\\ndata ass it as the reference data source\\nof this\\npipeline so I've uploaded the data now\\nwe need to split the text into smaller\\nchunks so that it could fit into the llm\\nthat we're going to use in the future so\\nyeah let's do that right now uh here\\nI'll be using the recursive character\\ntext splitter and I'll be joining both\\nof the components so now since we have\\nour smaller chunks of text we can uh\\nconvert it to uh numerical embeddings\\nwith the help of an embedding model so\\nuh let's do that uh we need a hugging\\nface inference API embeddings and I'll\\nput my API key and also the model name\\nso I've added the API key and the model\\nname here I'll be using this model\\nmentioned in the model name now since we\\nhave our embedding model defined we also\\nneed to convert those text information\\ninto embeddings and then we have to\\nstore it in a separate Vector store here\\nI'm going to use the chroma Vector store\\nand I'll be connecting\\nthis both\\ncomponents like this so as you can see\\nuh these all are the retriever\\ncomponents so whenever we ask a query to\\nthe rack pipeline the vector store\\nfetches the most relevant information\\nand passes it to the generator component\\nwhich generates a proper answer and\\ngives to us so let's start building the\\ngenerator component for that we need an\\nllm here I'll be using the Azure chat\\nopeni so you have to put the Azure chat\\nopeni API base the API key\\nthe type and the deployment name along\\nwith the model name so I'll fill all the\\nfields and unfortunately I cannot expose\\nthem in the video now since we have\\ndefined our llm we need to connect the\\ngenerator component with the Ral\\ncomponent which you can see on your\\nscreen so for that we need chains first\\nof all I will drag and drop the combine\\ndocs chain and after that uh I need to\\nhave the retrieval qm I will connect\\nboth of the components and I will\\nconnect the chroma to the Reet component\\nso this is how we have our generator\\ncomponent ready but before that we have\\nto connect the llm to the combine do\\nstream so I'll do that right now after\\nconnecting all the components we can\\nbuild the entire rack pipeline by\\nclicking on this buildt stack\\nbutton once the stack is built you can\\nchat with your application by clicking\\non the chat with stack button which is\\npresent just below the build stack\\nbutton it will notify you once the stack\\nis ready for\\ntesting so as you can see our stack is\\nready for testing and we are going to\\nchat with our application so let's ask\\nwhat are some of the values of the field\\nP class the field P class is present in\\nthe CSV file which I have uploaded for\\nreference so it answers in the G given\\ncontext the values of class are 1 and\\nthree so it automatically recognizes\\nthat the value class refers to P class\\nso yeah as you can see building this\\nentire application took me just a few\\nminutes so you can also build your\\napplication by using a separate CSV file\\nlet's say or we also have supports for\\nPDF and other types of data so you can\\nalso experiment with the embedding model\\nand the llm so yeah you can also try\\nbuilding your own stack I hope this\\nvideo was helpful if you like this video\\nplease let us know by hitting on the\\nlike button and subscribing to the\\nchannel we will release more videos in\\nthis series to solve new use cases till\\nthen stay tuned thank you\", mimetype='text/plain', start_char_idx=0, end_char_idx=4190, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-5.187207)]\n"
     ]
    }
   ],
   "source": [
    "# Örnek bir sorgu ile veri getirme ve sonuçları gösterme\n",
    "query = \"Which tool is mentioned in the video?\"\n",
    "retrieved_nodes = retriever.retrieve(query)\n",
    "print(\"Sorgu sonucu bulunan veriler:\", retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face'den bir dil modeli ayarlama\n",
    "llm = llms.HuggingFaceHubModel(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",  # Hugging Face üzerinden bir LLM\n",
    "    token=os.environ.get('HF_TOKEN')            # Hugging Face API anahtarını kullanarak\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt ekleme\n",
    "system_prompt = f\"\"\"\n",
    "<s>[INST]\n",
    "You are an AI Assistant.\n",
    "Please provide direct answers to questions.\n",
    "[/INST]\n",
    "</s>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorgu oluşturma ve pipeline'dan yanıt alma\n",
    "pipeline = generator.Generate(\n",
    "    question=query,            # Sorgu olarak \"query\" değişkeni kullanılıyor\n",
    "    retriever=retriever,\n",
    "    system_prompt=system_prompt,  # System prompt'u ekledik\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model yanıtı: \n",
      "        ANSWER: The tools mentioned in the video are Titanic dataset (for reference), RAG (Retrieval Augmented Generation), Tinybird, Hugging Face Inference API, Azure Chat Openi, Chroma Vector Store, and Combine Docs Chain.\n"
     ]
    }
   ],
   "source": [
    "# Yanıtı al ve göster\n",
    "response = pipeline.call()\n",
    "print(\"Model yanıtı:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing RAG Triad Evaluations...\n",
      "RAG Triad Değerlendirmesi: Context relevancy Score: 10.0\n",
      "This response meets the evaluation threshold. It demonstrates strong comprehension and coherence.\n",
      "Answer relevancy Score: 10.0\n",
      "This response meets the evaluation threshold. It demonstrates strong comprehension and coherence.\n",
      "Groundness score: 7.0\n",
      "This response does not meet the evaluation threshold. Consider refining the structure and content for better clarity and effectiveness.\n"
     ]
    }
   ],
   "source": [
    "# RAG Triad değerlendirme metriklerini al ve göster\n",
    "rag_evals = pipeline.get_rag_triad_evals()\n",
    "print(\"RAG Triad Değerlendirmesi:\", rag_evals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
